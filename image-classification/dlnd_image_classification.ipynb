{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:56, 3.01MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 10:\n",
      "Image - Min Value: 24 Max Value: 130\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 4 Name: deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAGq9JREFUeJzt3cmuI2mSHlAjnTPvEBEVmdVdJQHatB6hH0DvL2gjQAK0\nkNTVlV3VmRlxR850aqGddma41SkYztkbjHT+7h999U1ut1sAAD1Nf+sPAAD87Qh6AGhM0ANAY4Ie\nABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEP\nAI3NfusP8Lfyj//pH2+VucmYH5tex8qqKKyK9XZb2vX4+FiaG8f8d3t9fS3tmk7yF2S1mJd2Hd53\npbn1YpWeWSxq/6eX2/ztuZznP19ExOFwKcycaruO+9LcZDpJz9xt70q7lqv8dbxczqVdp1PtOi6X\n6/TMr788lXb99a8/p2eG2bK0azLU7ulhGNIz5/O/3W/2/fv30q5/+dM/5w/+/8MbPQA0JugBoDFB\nDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGNt2+uOp7fS3HLI\nX5LxVirKi6HQ0nSLa2nX+67WKDefL9Iz602ttepYaDWbzGrFTnePtVazxbRwy4y1drLFNN8c+HBX\na6/bv+Xbyaa32llcr2vno9IRebrUrn0UxjabfJtcRMRkWnt+xC1/Re7uN6VVv/ySv8/Ol3wjYkTE\nUHz/vBWew9X2ukqr52z228WtN3oAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBo\nTNADQGOCHgAaE/QA0FjbUptq+cu10JxxOR5Lu1arfMHEMOaLcCIi1utaicvDw0N65u39vbTrdDmk\nZ5abWonLel4rVhkK/SPHfe0sTif5Zc9P30q7xmu+3GM+r53Fc62HKIYh/14yDENp12yWnzue8uc3\nonbt/+9c/kIWulgiImK5zJdbXfa1UptKYUzVpVi8U/mMk0nx4H8Ab/QA0JigB4DGBD0ANCboAaAx\nQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNtW2vWxea4SIizod8E910WruM\ntTajWrPTMKv9pxtv+ea1SaF1LSJivc030Z0up9Kuxbz2m41j/rvdf3os7ZoN+Watn/78l9Ku5TJ/\nv0yHWnvdpHCmIiJiyN8vw7x27s+Fc/X+9lbatZjWGvbmhQbG6nPg4THffnm61K7H8VR7xlXaFGez\n2nPgWGgsvb+/L+36CN7oAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCN\nCXoAaEzQA0BjbUtt5rN1aW4s/PXZPtR27ffv+ZnDobTr9fWlNDeJfInLeKuVUlzGfInLdlu79reo\nFausN/ninaFYoHMt/A+///pjaVflUfD6UistuU2LpSVD/nqcb/kzFRFxLRTvfP3919KuRdRKbcZr\n/jqOlQdcRJxP+et4vdau/ThWyr4iLpf8vmqpzemULz3abGpFax/BGz0ANCboAaAxQQ8AjQl6AGhM\n0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0BjbdvrYjIvjd3dLdMzq1lt13ye\nnzuPu9quWe0/3el8zA9Nau1TY6ExbLWuNUKdD4XvFRHv+31+5lC7Hpu7u/TMOK3d0u9v+e+1fngs\n7dq9fyvNxZhvUrx/uC+tOhbaySqNZhERt1vtfCwW+WfVsdh+uVrnd41jrSFyGGrP00pbXuUaRkQs\nl/m58/lc2vURvNEDQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQ\nmKAHgMYEPQA01ra97nwdS3OV8q/DpdYINb3l24zGc23X8Va7HvPlOj0zLBalXXeFtrZJDKVd12vx\n6Bca9maz2md8fnpNz0yutVa+w9tbeub+Pv97RUR8uau13k3GfDvcMNaa4S6F4rXdrnZvvl/yrWsR\nEZ8e8+dqOq+9250L135daAKNiNi91VreJtP8b30pXvtCkWIUj+KH8EYPAI0JegBoTNADQGOCHgAa\nE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABprW2pzuxVaByLieMqXgmyW89Ku7SZf\nGHOd177XdKh9xtlqk575y8+/lHbtju/pme3mobRrNV+V5i7nfWFX8TYb84Ubk2J50Xqeb9y4FguW\n7tb5MxURcdrni1VOh1rJz1AoIlqt8/dzRMS1WqxSmNlsa9f+cMz/1g8PtdKj97fa82O92qZnbmPt\nXfdaaLUZJ7Vn90fwRg8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQ\nA0Bjgh4AGhP0ANBY2/a6dbVJ6pRvaRqGfNNVdW59V2uEmi2WpbnzmG81m89rTXm36zU98/r9qbRr\ndqt9xsU0/xm3D7VrP0zyt+f+eC7t+vHrY3rmUGjwioi4XGufcVY4V5XWtYiI9TLfbjgr9clFTCf5\neywi4nLJX8fn53wDYETE4ZC/jvP5orRrmBXfPwvtcLN5bddwy8+dx/yz46N4oweAxgQ9ADQm6AGg\nMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjbUttdlsNqW5p8N7euZyqZV7\n3G75y18t0LnVPmLsdvv0TPUzrirFO+dakcj1tCvNTeb5fb9//ENp1//86af0zNdPD6Vdnz9/Ts+8\n7GslHbt9rdTmXChxmS1q5UWVU3Uda2dxLM7t9/l7c7msFSxViqrGa+09clYstRkLpTHDtBaBl0u+\nHGiMWnnRR/BGDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOC\nHgAaE/QA0Fjb9rrL5VKam0zyDUPnU77JKCLi5SU/NzzUWvkm01rTWES+9m69Xpc2nXf5RrmvX/Kt\naxERw6x2PubX/Gc8vbyWdu1f8+1k26i1k/3808/pmaddrYVuulyV5uarRXpmvBXbDQtNefvjobRr\nMa21Pd7d3aVnttttaddL4Qwv5rXnwO69dh2fn9/SM5fC7xwRMV/kz+LlVHvmfARv9ADQmKAHgMYE\nPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI21ba+rqrQ7HXf5\n1qSIiMsl39J0Otea8ooFWTFWyr+G2v/Hx4fH9Mz5cCztWhUvyO2Qb6/7yz/9qbTr06e/T88c3p5K\nu56fX9Izb+d8s2FExMPva4+dyzR/GE/FFsvZMt9OtijMREQcXt5Lcw8PD+mZXaEhMiJiPs//ZkPx\nObBczktz45j/raf5stKIiFgs8p/xevvt3qu90QNAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCN\nCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxtqW2lwLBQcREbPCX59hXitImQ7L9My5WCSyLn7G1aJQZlEo\nwIiIuJ3zpSWv77VCoXGofcbH5SY9s9vny4siIr7/6af0zGw8l3at1vmzuFnlZyIiPn39oTT311//\nmp65Re1+ifM1PTIpFqTMivfmbpcvw5kV7831apWeeXt9Lu2aVctwFvlSodOp0toVcTzmy8WWi3Vp\n10fwRg8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0\nANBY2/a6y6nWGHYbChVUxb9L4y3fWnWb1JbtC21LERE/PG7TM3f3+ZmIiD//Od9Odp3XKsOuhaar\niIjLOt9et1g/lnZ9+2//Iz0zvdTa636/yTdr3X25K+26Fp86i03+2p+L5z6ulda7WhPa9q7Wavb6\n+pqemc1r5/58OaZnruf8TETE5Fpr8xsKz8bzqXa/XK75czWfaa8DAP4GBD0ANCboAaAxQQ8AjQl6\nAGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNtS21uR72tcEhX6gwLxZFVIxjrThjvNbK\nG97fdumZU7HM4lL5boXfKyLiMqmUlkS8n/NlFl8//1DatVrmy4Fu09q5vxUKWYZ57Roej2+lufMp\n/91u10tp12xaOFe32vU47WsFXKtCMdOsWIp1i/x3u1QLhcbaPT2NfMHVbChGYOF8HPbFTPoA3ugB\noDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaa9te\nN7nU2toux8Jc8SoulvnB+brW7DTM5qW5mOQboSZR+4yfPn1Jz/z8y7fSrs39pjS3KFyP7f26tOtL\n4Xq8P/1radflnG9Qe3v5tbTr0+9rbX5Phda7ZbGdbD7N/87jpdYs+f5ea6/74x/+WJqr+OXnn9Mz\ni1mt1XM5r92bh8NzemZyq+XEtfBbT+e15+JH8EYPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0A\nNCboAaAxQQ8AjQl6AGhM0ANAY4IeABprW2qzmNcKFcbpLT1zu+VnIiLG8ZKemS+K5TRFl8s1PbNa\n1kopYpL/3/n1h6+lVdPIX/uIiMUqX0xxHU+lXbPCWfzd50+lXd/f82U4T993pV13jw+luek1fxbv\n7u5Lu66nfNnJpPYYiO28Vnr0/vSanlkul6Vdccl/ueVQe1a9Pj+V5k6H/H12Ptbuzest/6waigVL\nH8EbPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNAD\nQGNt2+vmq7vSXKVg6HB4L+06X/bpmf0+3+AVETGd1pqkxsK6/a7WCLV6yLea/f0f/66067h/Ls3t\nDm/pmbtVrTFstcrPvP76UtoVY35kcq09Pp5/zbeuRUScdvnGwZdLbde60H45K95ju7fa8+P5kG95\n+/z5c2nXcpo/w0/fv5V2/frte2lus81/t2Wx5fRwrjyHi/WGH8AbPQA0JugBoDFBDwCNCXoAaEzQ\nA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGNt2+uG1X1p7m33c3pmusi3akVE\nrNaFy38p1IxFxGJe+6mvk/x/wf2h1l737Xu+tWoyn5R2bVa1/7jPL/lGrr//8XelXf/wH/+Qnvmv\n/7nWGLZ7zZ+rw7nWxnW+5BsAIyKWw5CeeS02w10K9/TkVjuL77tdaW46zZ/hyVg79/N5vpnvfDqX\ndk2idh2Haf58LGqFg3G6VM5+7Xt9BG/0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAa\nE/QA0JigB4DGBD0ANCboAaCxtqU210LBQUTEcrNOz6y2tbKC9Tz/P+v7T7VCkDjXinfimh+Z1S59\nnE75Mpzj60tp13rYluYux/xnfH+v/WaPd/nGjdV6Udo1edmnZy7H2pmazmpz28dNeubnf3kt7Xq8\ne0jP7N/z1zAi4nyqXY/5Mv9bv77Xrsdmm7/2l2KJy1go0oqIuBXSbDGpReDlrXBPn3+792pv9ADQ\nmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI21ba+b\nzWvNSfu3fJPUUKl4i4jlLN9Otl3VWtemp7E0F2P+u03ntfq6+02+MWy+yF/DiIjlUPuP+/XTl/TM\nZpVv/oqI2B0O6Zn3Xa1BbVY4i7NzaVVsNrWGvd/98Jieefr2rbTrFvnnwGSoPXNO19q9ebvl781h\nUntWTSL/Y4/z2r15nhZb76b573YrNuwNs/zceKld+4/gjR4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEP\nAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANNa21Ga45AtBIiJWk/x/n8tLrazgcD7ld51rBRjr\nofZT3+KWnqlWNywW+bKTh4f72rJiucfnT/ninUXx2u9en9Mz4612Pmaz/GeczfPFLxER17H2fvHy\nnC9WmU6XpV0//PhDemY2q5X1/PTtv5Tm5otVemZY14pmTpP8b719uCvt2m5rJVCn8y49s3vNz0RE\nLFf5c3XYFYvFPoA3egBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4Ie\nABoT9ADQmKAHgMbattfd9m+luek535B1u9Zaid73x/TMUGh4i4hYr9aluWuhDe3luC/tms3zx3Ec\na9d+vOabAyMivr2+pGc+FRrvIiKmk0l65suXz6Vdp1O+pfCUvxQREfF2qLXevQz5+2W9qTWhPb08\npWeut/w1jIgY1rV7elpoojtG7dpXzMbartulNjeZ5K//3V3tufj910o7av5+/ije6AGgMUEPAI0J\negBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABpr214Xl3zTVUTE\nfJpvGNpuao1h10KZ0fFWa13b7WuNcvNFviFru92Wdk2HIT1zi1pj2HqxLM398JBvoluta7u+ffue\nnhmGWkPWZpNvUPt3D/elXf/9f/3v0txqs0rPnI+1Fsv9KX+/XGtHMaLwzImIGAttbUPx1W6c5Fsi\nx9u1tKv6GSvlcJVnTkTEcpV/Lr6/1Z7BH8EbPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCY\noAeAxgQ9ADQm6AGgMUEPAI0JegBorG2pzfl8Kc1tH9aFXbUCnXGaL1Q4XmulNutJrbzhes0XU1zP\n+QKMiIjj9ZyeedjUCnQei4Usy8Jvdiuexcslf+2Xy1qBzmqVL4x5LZ7781gr95gs8tfxYbMp7Trt\n8t9t91Ir0Hm4r33G+SpfRDQsawU6p8Jz5+3tubTrjz/+XWnubfeUnjkdDqVdi0X+2v+WvNEDQGOC\nHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA01ra9Lmbz\n0tg4vaVnLmO+dS0i4hb5zzgbai10i1mtbel0zrdWnU75axgRcbrm28nmk9p/1dnnT6W5a6GJbpjV\nfrPlMt8oN5nWzuL2Lr/r6dfX0q5//x9+KM1Nh/y52m6KLWO3fAPj4V93pVV3D4+luWXhXE1ntftl\ntczvuixrTZuLZe03W435M3w81M5wpdVzNvvt4tYbPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0\nANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBorG2pzSnfSREREdNhnZ5ZLmsFOqdjvvRhtVyWdq3X+cKH\niIjXX9/SM5N5rcRlNZ2kZ8bDvrTrcjmW5oZ5/r/x+XQo7fq02qRnvp9q1+N9zM/d/3hX2jU/1kpL\nxnyfUBxPtaKZ2zRfWvK7H7+Udp0Lz4GIiBjzJT/nfe3cz1f5e3MyyV/DiIj5vPY8PX4vPPRv/3YR\nOMxqZV8fwRs9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6\nAGhM0ANAY23b646VqquImM7yLW+zqO2qtFZNbrUGpPOl9hkXq0JbXqGFLiJiEfm59aLWdDUMtf+4\nt0J73dvza2nX/Jpv4xpvtd/5n/7yS3rm8x++lnadDrVWs+N7voluMqvtul7z99lsVmttnIy1s3gp\n3NOnS60p71a4p4/HWnPgfp9vzIyImA3563+5FBv2FvmcGG/vpV0fwRs9ADQm6AGgMUEPAI0JegBo\nTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY23b61abdWnuZZdvGFpV29oK\nn3EyqbXXXcZ8E1pExHK1Sc8cz+fSrrHQzLfcbmu7SlMRp90xPXO91hqyxkn+Op6L7WQP95/SM7dL\n7fFxvNYa9o6Rv46f17XnwKfCvfn2XGsnez7nz1RExOmUnzsVWyyX2/z1+PL5S2nX4XAozd0Kz4/K\nNYyIOJ/zT5BKu95H8UYPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM\n0ANAY4IeABprW2ozn9W+WqUy5lrrtIldocxis1iUdm3v70tz+1O+BGMy1v4/Xsd8icvuWCvQmS9r\n1/F6LlyPSe2ALLfL9Mz8Ui0UyhduTK61e2x3qBXvLAq/2W2slUCtVvP0zHuxvGgYap9xGPLn6nqs\n1TlVSly26/z5jYjYve1Lc7fCc2ccayU/53P+tx6mtevxEbzRA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm\n6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANNa2vW52qzWGzYb8f59J1BqhboWuvMms\n2AxXK8iK2yR/RFabdW1X5FvNDsddaVe8vtfmLvnP+LCptVa97vLthmPx3B8O+V3z4uPjNtbul7Fy\niOe1++VyybeaXYpNaF9/+FKa2x7zjYPHf/5raddYKOarXMOIiNOp1l43n+WfO5vtqrSr0kT39D1/\nj30Ub/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoLG2\npTabQhlLREShZyYm01qRyG2+SM+Mk1o7zalYMHEd89dxOs2XbURE3Cb5uemiVhgzn9fOxzDk58Zr\nrcTl6emQnpnOa9d+vcoXgkyKrwmLYsHSpFBqM4navXkstLhMFrUztV7XzvCv35/TM5v1trRrWShm\nul5rhVOzWe0Mx6Ryn9Xuzcpc7SR+DG/0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAa\nE/QA0JigB4DGBD0ANCboAaAxQQ8AjU1ut2KVFADw/z1v9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoA\naEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0A\nNCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4A\nGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8A\njQl6AGjs/wCMj7S6AwR1rgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15627109b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 10\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    a = 0\n",
    "    b = 1\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (x - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "    one_hot_x = lb.transform(x)\n",
    "    #print (x)\n",
    "    #print (one_hot_x)\n",
    "    return one_hot_x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    #print (image_shape)\n",
    "    return tf.placeholder(tf.float32,shape =[None,*image_shape],name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    #print (n_classes)\n",
    "    return tf.placeholder(tf.float32,shape =[None,10],name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "   \n",
    "    return tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 32, 32, 5), dtype=float32) 10 (2, 2) (4, 4) (2, 2) (2, 2)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    #print(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    filter_size_height = conv_ksize[0]\n",
    "    filter_size_width = conv_ksize[1]\n",
    "    x_tensor_depth = int(x_tensor.get_shape()[3])\n",
    "    #print(type(x_tensor_depth))\n",
    "    #print (filter_size_height,filter_size_width,x_tensor_depth)\n",
    "    weight = tf.Variable(tf.truncated_normal([filter_size_height, filter_size_width, x_tensor_depth, conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, [1,conv_strides[0],conv_strides[1],1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,[1,pool_ksize[0],pool_ksize[1],1],[1,pool_strides[0],pool_strides[1],1],padding='SAME')\n",
    "    print (x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 32, 32, 3), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 8, 8, 64), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 2, 2, 64), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"Placeholder:0\", shape=(?, 32, 32, 3), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"MaxPool_3:0\", shape=(?, 8, 8, 64), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"MaxPool_4:0\", shape=(?, 2, 2, 64), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (5, 5)\n",
    "    conv_strides = (2,2)\n",
    "    pool_ksize = (3, 3)\n",
    "    pool_strides = (2,2)\n",
    "    n_class = 10\n",
    "    \n",
    "    conv_layer = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    #  Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat_x = flatten(conv_layer)\n",
    "    \n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    full_layer = fully_conn(flat_x,32)\n",
    "    full_layer = fully_conn(full_layer,n_class)\n",
    "    #full_layer = fully_conn(full_layer,n_class)\n",
    "    \n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    drop_layer = tf.layers.dropout(full_layer,rate = keep_prob)\n",
    "    out = output(drop_layer, n_class)\n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    train_feed_dict = {\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability     \n",
    "    }\n",
    "    session.run(optimizer, train_feed_dict)\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "    valid_accuracy = session.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob:1.})\n",
    "    print ('loss:',loss,'valid_accuracy',valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.30733 valid_accuracy 0.1192\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.28907 valid_accuracy 0.117\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.26558 valid_accuracy 0.1676\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.23254 valid_accuracy 0.194\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.18858 valid_accuracy 0.213\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 2.15051 valid_accuracy 0.2306\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 2.12575 valid_accuracy 0.2364\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 2.0875 valid_accuracy 0.2592\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 2.0611 valid_accuracy 0.2832\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 2.03713 valid_accuracy 0.2736\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.991 valid_accuracy 0.3112\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.9538 valid_accuracy 0.3218\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.93451 valid_accuracy 0.3232\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.91373 valid_accuracy 0.3252\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.85616 valid_accuracy 0.3404\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.81648 valid_accuracy 0.3444\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.81386 valid_accuracy 0.3358\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.803 valid_accuracy 0.341\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.71069 valid_accuracy 0.3842\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.67832 valid_accuracy 0.3768\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.63942 valid_accuracy 0.385\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.57206 valid_accuracy 0.399\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.54384 valid_accuracy 0.4124\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.49073 valid_accuracy 0.423\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 1.4541 valid_accuracy 0.4298\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.43319 valid_accuracy 0.44\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.45182 valid_accuracy 0.4316\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.3734 valid_accuracy 0.456\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 1.35454 valid_accuracy 0.4632\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 1.35371 valid_accuracy 0.462\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 1.3633 valid_accuracy 0.46\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 1.34392 valid_accuracy 0.461\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 1.30995 valid_accuracy 0.4704\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 1.27305 valid_accuracy 0.4816\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 1.26665 valid_accuracy 0.481\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 1.24839 valid_accuracy 0.4836\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 1.22236 valid_accuracy 0.4914\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 1.22365 valid_accuracy 0.49\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 1.22804 valid_accuracy 0.4854\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 1.21296 valid_accuracy 0.4884\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 1.17054 valid_accuracy 0.4964\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 1.16957 valid_accuracy 0.5074\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 1.19744 valid_accuracy 0.4932\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 1.13771 valid_accuracy 0.5022\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 1.10919 valid_accuracy 0.5096\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 1.10137 valid_accuracy 0.5068\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 1.09134 valid_accuracy 0.5138\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 1.07014 valid_accuracy 0.5166\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 1.07176 valid_accuracy 0.5176\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 1.10412 valid_accuracy 0.5054\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 1.08119 valid_accuracy 0.5048\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 1.06533 valid_accuracy 0.508\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 1.02973 valid_accuracy 0.5132\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 1.01745 valid_accuracy 0.5148\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 1.00838 valid_accuracy 0.5198\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 1.01129 valid_accuracy 0.5226\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.998629 valid_accuracy 0.5254\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.987656 valid_accuracy 0.527\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.968222 valid_accuracy 0.529\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.954005 valid_accuracy 0.5262\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.958084 valid_accuracy 0.52\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.947536 valid_accuracy 0.5174\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.922095 valid_accuracy 0.5222\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 1.01582 valid_accuracy 0.5012\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.964383 valid_accuracy 0.5262\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.941415 valid_accuracy 0.5336\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.966097 valid_accuracy 0.5082\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.974714 valid_accuracy 0.4976\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.938096 valid_accuracy 0.5178\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.873817 valid_accuracy 0.5274\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.893075 valid_accuracy 0.5186\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.831818 valid_accuracy 0.538\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.85232 valid_accuracy 0.524\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.889088 valid_accuracy 0.516\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.823038 valid_accuracy 0.5316\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.793339 valid_accuracy 0.5364\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.789701 valid_accuracy 0.5318\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.784954 valid_accuracy 0.5304\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.787727 valid_accuracy 0.5262\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.816603 valid_accuracy 0.5184\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.807653 valid_accuracy 0.5254\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.742399 valid_accuracy 0.5382\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.75164 valid_accuracy 0.5344\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.743795 valid_accuracy 0.5354\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.729145 valid_accuracy 0.5388\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.730432 valid_accuracy 0.5382\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.738629 valid_accuracy 0.5348\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.736137 valid_accuracy 0.53\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.72855 valid_accuracy 0.5238\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.747154 valid_accuracy 0.5128\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.732495 valid_accuracy 0.5144\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.668157 valid_accuracy 0.5326\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.665286 valid_accuracy 0.5358\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.644333 valid_accuracy 0.5414\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.639545 valid_accuracy 0.5382\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.627592 valid_accuracy 0.537\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.619762 valid_accuracy 0.5348\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.610146 valid_accuracy 0.5336\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.605218 valid_accuracy 0.5316\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.598 valid_accuracy 0.5302\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.30247 valid_accuracy 0.1008\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 2.29983 valid_accuracy 0.1112\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 2.29171 valid_accuracy 0.115\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 2.29344 valid_accuracy 0.111\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 2.28058 valid_accuracy 0.1208\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.27616 valid_accuracy 0.1022\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 2.27485 valid_accuracy 0.1258\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 2.27344 valid_accuracy 0.1526\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 2.25795 valid_accuracy 0.1508\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 2.26394 valid_accuracy 0.138\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.24723 valid_accuracy 0.1584\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 2.2413 valid_accuracy 0.1596\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 2.24837 valid_accuracy 0.1566\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 2.2292 valid_accuracy 0.1542\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 2.24616 valid_accuracy 0.1538\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.22344 valid_accuracy 0.1582\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 2.22341 valid_accuracy 0.163\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 2.23757 valid_accuracy 0.1642\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 2.21184 valid_accuracy 0.1648\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 2.22145 valid_accuracy 0.1678\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.20862 valid_accuracy 0.1654\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 2.18859 valid_accuracy 0.1696\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 2.1897 valid_accuracy 0.1714\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 2.14506 valid_accuracy 0.1756\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 2.12535 valid_accuracy 0.2026\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 2.13667 valid_accuracy 0.186\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 2.08265 valid_accuracy 0.2106\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 2.01309 valid_accuracy 0.2414\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 1.87261 valid_accuracy 0.2842\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 1.79211 valid_accuracy 0.3282\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.71165 valid_accuracy 0.3472\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 1.69347 valid_accuracy 0.3594\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 1.6137 valid_accuracy 0.3854\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 1.58503 valid_accuracy 0.3942\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 1.59357 valid_accuracy 0.4054\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.52304 valid_accuracy 0.4172\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 1.54951 valid_accuracy 0.4172\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 1.49457 valid_accuracy 0.4424\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 1.45223 valid_accuracy 0.4466\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 1.44965 valid_accuracy 0.4546\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.40306 valid_accuracy 0.4686\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 1.44514 valid_accuracy 0.447\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 1.39865 valid_accuracy 0.4768\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 1.33934 valid_accuracy 0.493\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 1.3463 valid_accuracy 0.4886\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.33121 valid_accuracy 0.4888\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 1.37783 valid_accuracy 0.4718\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 1.32252 valid_accuracy 0.5052\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 1.27552 valid_accuracy 0.5088\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 1.256 valid_accuracy 0.5198\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.24686 valid_accuracy 0.5256\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 1.33924 valid_accuracy 0.4858\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 1.27498 valid_accuracy 0.518\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 1.21828 valid_accuracy 0.5302\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 1.22675 valid_accuracy 0.5304\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.19935 valid_accuracy 0.538\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 1.25732 valid_accuracy 0.5212\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 1.2111 valid_accuracy 0.5334\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 1.18779 valid_accuracy 0.5368\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 1.17666 valid_accuracy 0.5436\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.17406 valid_accuracy 0.5494\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 1.20476 valid_accuracy 0.5344\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 1.16771 valid_accuracy 0.5494\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 1.15532 valid_accuracy 0.5474\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 1.16909 valid_accuracy 0.543\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.14952 valid_accuracy 0.5562\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 1.17631 valid_accuracy 0.5384\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 1.13986 valid_accuracy 0.5556\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 1.10598 valid_accuracy 0.5584\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 1.08507 valid_accuracy 0.562\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.12244 valid_accuracy 0.5592\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 1.14705 valid_accuracy 0.5504\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 1.11158 valid_accuracy 0.5622\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 1.09841 valid_accuracy 0.5624\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 1.05107 valid_accuracy 0.575\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.10108 valid_accuracy 0.5644\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 1.11376 valid_accuracy 0.5596\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 1.09028 valid_accuracy 0.5632\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 1.07334 valid_accuracy 0.569\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 1.02913 valid_accuracy 0.5802\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.07896 valid_accuracy 0.5666\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 1.07236 valid_accuracy 0.5684\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 1.04726 valid_accuracy 0.5708\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 1.03752 valid_accuracy 0.5812\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 0.999263 valid_accuracy 0.5844\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.06036 valid_accuracy 0.5726\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 1.05073 valid_accuracy 0.5768\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 1.01236 valid_accuracy 0.5796\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 1.01357 valid_accuracy 0.5788\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 0.969584 valid_accuracy 0.5902\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.0422 valid_accuracy 0.5752\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 1.02662 valid_accuracy 0.5818\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 0.982514 valid_accuracy 0.5852\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 0.99309 valid_accuracy 0.587\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 0.944421 valid_accuracy 0.5912\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.02654 valid_accuracy 0.5744\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 1.00929 valid_accuracy 0.585\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 0.95862 valid_accuracy 0.5924\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 0.972264 valid_accuracy 0.589\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 0.924353 valid_accuracy 0.591\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 0.997921 valid_accuracy 0.5818\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 0.993174 valid_accuracy 0.5856\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 0.953517 valid_accuracy 0.589\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 0.952171 valid_accuracy 0.5904\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 0.912859 valid_accuracy 0.5908\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 0.957872 valid_accuracy 0.5952\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 0.950151 valid_accuracy 0.5972\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 0.9229 valid_accuracy 0.5988\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 0.941298 valid_accuracy 0.5902\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 0.892525 valid_accuracy 0.5952\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 0.93808 valid_accuracy 0.6016\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 0.931401 valid_accuracy 0.6014\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 0.900663 valid_accuracy 0.6016\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 0.917708 valid_accuracy 0.597\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 0.87793 valid_accuracy 0.5954\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.920128 valid_accuracy 0.6008\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 0.912373 valid_accuracy 0.5984\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 0.883622 valid_accuracy 0.6054\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 0.899123 valid_accuracy 0.6\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 0.865166 valid_accuracy 0.5962\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.900989 valid_accuracy 0.605\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 0.893088 valid_accuracy 0.6026\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 0.863079 valid_accuracy 0.6098\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 0.879851 valid_accuracy 0.6024\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 0.847273 valid_accuracy 0.5994\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.882337 valid_accuracy 0.6084\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 0.874863 valid_accuracy 0.6038\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 0.846867 valid_accuracy 0.6102\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 0.862947 valid_accuracy 0.6002\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 0.830665 valid_accuracy 0.602\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.873869 valid_accuracy 0.6092\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 0.857675 valid_accuracy 0.6084\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 0.833179 valid_accuracy 0.614\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 0.85272 valid_accuracy 0.604\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 0.807928 valid_accuracy 0.607\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.863621 valid_accuracy 0.6098\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 0.844477 valid_accuracy 0.6088\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 0.82347 valid_accuracy 0.6172\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 0.844628 valid_accuracy 0.6046\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 0.789137 valid_accuracy 0.6146\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.849877 valid_accuracy 0.6118\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 0.833367 valid_accuracy 0.6118\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 0.816213 valid_accuracy 0.6138\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 0.85709 valid_accuracy 0.6056\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 0.802544 valid_accuracy 0.6064\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.838769 valid_accuracy 0.617\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 0.82681 valid_accuracy 0.6086\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 0.802117 valid_accuracy 0.6146\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 0.85259 valid_accuracy 0.605\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 0.798168 valid_accuracy 0.605\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.83874 valid_accuracy 0.617\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 0.806177 valid_accuracy 0.6194\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 0.811137 valid_accuracy 0.61\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 0.809856 valid_accuracy 0.6084\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 0.776091 valid_accuracy 0.6128\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.8118 valid_accuracy 0.6202\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 0.800691 valid_accuracy 0.617\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 0.807959 valid_accuracy 0.6058\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 0.83644 valid_accuracy 0.5882\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 0.794639 valid_accuracy 0.6102\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.816583 valid_accuracy 0.6184\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 0.811189 valid_accuracy 0.614\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 0.839562 valid_accuracy 0.5982\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 0.854835 valid_accuracy 0.5934\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 0.772884 valid_accuracy 0.6108\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.820829 valid_accuracy 0.6168\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 0.78977 valid_accuracy 0.6234\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 0.776397 valid_accuracy 0.6162\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 0.823309 valid_accuracy 0.6022\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 0.768447 valid_accuracy 0.6078\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.788649 valid_accuracy 0.6234\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 0.771442 valid_accuracy 0.626\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 0.758437 valid_accuracy 0.6222\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 0.8003 valid_accuracy 0.6026\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 0.739292 valid_accuracy 0.6144\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.772631 valid_accuracy 0.6256\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 0.75769 valid_accuracy 0.629\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 0.745084 valid_accuracy 0.6218\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 0.787596 valid_accuracy 0.6036\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 0.720118 valid_accuracy 0.617\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.764199 valid_accuracy 0.6228\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 0.748492 valid_accuracy 0.628\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 0.731581 valid_accuracy 0.6256\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 0.760964 valid_accuracy 0.6112\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 0.703475 valid_accuracy 0.621\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.759049 valid_accuracy 0.6244\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 0.740752 valid_accuracy 0.628\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 0.722278 valid_accuracy 0.6256\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 0.743796 valid_accuracy 0.6172\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 0.69539 valid_accuracy 0.6198\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.742614 valid_accuracy 0.6252\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 0.733651 valid_accuracy 0.6282\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 0.715923 valid_accuracy 0.629\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 0.729427 valid_accuracy 0.623\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 0.67736 valid_accuracy 0.624\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.717047 valid_accuracy 0.6332\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 0.72952 valid_accuracy 0.6286\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 0.708482 valid_accuracy 0.6298\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 0.72192 valid_accuracy 0.6216\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 0.659166 valid_accuracy 0.629\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.694945 valid_accuracy 0.635\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 0.721845 valid_accuracy 0.629\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 0.698216 valid_accuracy 0.6312\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 0.71258 valid_accuracy 0.6212\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 0.646653 valid_accuracy 0.6328\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.681335 valid_accuracy 0.6366\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 0.716766 valid_accuracy 0.6226\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 0.692869 valid_accuracy 0.6324\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 0.702114 valid_accuracy 0.6194\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 0.63685 valid_accuracy 0.631\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.674475 valid_accuracy 0.6334\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 0.717788 valid_accuracy 0.6204\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 0.687995 valid_accuracy 0.6336\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 0.689944 valid_accuracy 0.623\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 0.64273 valid_accuracy 0.6304\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.689272 valid_accuracy 0.6278\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 0.712181 valid_accuracy 0.6198\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 0.668327 valid_accuracy 0.6368\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 0.686057 valid_accuracy 0.6268\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 0.667801 valid_accuracy 0.6176\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.697043 valid_accuracy 0.6286\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 0.694002 valid_accuracy 0.6342\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 0.694832 valid_accuracy 0.624\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 0.655496 valid_accuracy 0.6334\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 0.605939 valid_accuracy 0.633\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.68126 valid_accuracy 0.6308\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 0.679738 valid_accuracy 0.635\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 0.65803 valid_accuracy 0.6326\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 0.642784 valid_accuracy 0.6366\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 0.613331 valid_accuracy 0.6272\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.667476 valid_accuracy 0.6314\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 0.667269 valid_accuracy 0.6364\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 0.642855 valid_accuracy 0.6388\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 0.642679 valid_accuracy 0.6372\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 0.601487 valid_accuracy 0.63\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.641285 valid_accuracy 0.6358\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 0.66565 valid_accuracy 0.643\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 0.639917 valid_accuracy 0.6374\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 0.626865 valid_accuracy 0.641\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 0.576545 valid_accuracy 0.6378\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.618563 valid_accuracy 0.6426\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 0.665727 valid_accuracy 0.6404\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 0.628908 valid_accuracy 0.6384\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 0.615509 valid_accuracy 0.6434\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 0.564885 valid_accuracy 0.6402\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.609057 valid_accuracy 0.6398\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 0.659304 valid_accuracy 0.6428\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 0.612349 valid_accuracy 0.6396\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 0.604782 valid_accuracy 0.643\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 0.554731 valid_accuracy 0.6438\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.60154 valid_accuracy 0.6394\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 0.644612 valid_accuracy 0.641\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 0.59612 valid_accuracy 0.6376\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 0.592785 valid_accuracy 0.6418\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 0.546988 valid_accuracy 0.6436\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.588219 valid_accuracy 0.6404\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 0.634545 valid_accuracy 0.6394\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 0.587992 valid_accuracy 0.6376\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 0.585321 valid_accuracy 0.6426\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 0.538977 valid_accuracy 0.644\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.5767 valid_accuracy 0.6418\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 0.623718 valid_accuracy 0.6394\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 0.579204 valid_accuracy 0.6386\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 0.577229 valid_accuracy 0.6412\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 0.528299 valid_accuracy 0.643\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.570991 valid_accuracy 0.6394\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 0.612315 valid_accuracy 0.6378\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 0.575093 valid_accuracy 0.6408\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 0.569942 valid_accuracy 0.6414\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 0.520522 valid_accuracy 0.6422\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.561278 valid_accuracy 0.6406\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 0.600085 valid_accuracy 0.6372\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 0.569029 valid_accuracy 0.6408\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 0.564155 valid_accuracy 0.6388\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 0.515032 valid_accuracy 0.646\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.555454 valid_accuracy 0.6386\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 0.593807 valid_accuracy 0.639\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 0.563714 valid_accuracy 0.642\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 0.556894 valid_accuracy 0.6376\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 0.513364 valid_accuracy 0.6452\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.547521 valid_accuracy 0.6364\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 0.597602 valid_accuracy 0.636\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 0.574126 valid_accuracy 0.638\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 0.550651 valid_accuracy 0.6412\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 0.538247 valid_accuracy 0.6376\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.574881 valid_accuracy 0.6346\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 0.609618 valid_accuracy 0.63\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 0.550653 valid_accuracy 0.64\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 0.535828 valid_accuracy 0.649\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 0.48802 valid_accuracy 0.6514\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.551151 valid_accuracy 0.6368\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 0.59644 valid_accuracy 0.6346\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 0.573671 valid_accuracy 0.634\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 0.531055 valid_accuracy 0.643\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 0.486617 valid_accuracy 0.6536\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.534726 valid_accuracy 0.6382\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 0.602313 valid_accuracy 0.6286\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 0.570245 valid_accuracy 0.6338\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 0.561072 valid_accuracy 0.631\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 0.494827 valid_accuracy 0.6498\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.52049 valid_accuracy 0.6334\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 0.6126 valid_accuracy 0.6296\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 0.551349 valid_accuracy 0.6332\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 0.580567 valid_accuracy 0.6352\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 0.480966 valid_accuracy 0.6486\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.541467 valid_accuracy 0.635\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 0.61456 valid_accuracy 0.6284\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 0.607514 valid_accuracy 0.6186\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 0.643797 valid_accuracy 0.625\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 0.563089 valid_accuracy 0.634\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.529805 valid_accuracy 0.6446\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 0.558708 valid_accuracy 0.6396\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 0.578655 valid_accuracy 0.6286\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 0.622138 valid_accuracy 0.621\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 0.525044 valid_accuracy 0.63\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.546503 valid_accuracy 0.6384\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 0.550305 valid_accuracy 0.6384\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 0.518403 valid_accuracy 0.6446\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 0.61669 valid_accuracy 0.6182\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.565769 valid_accuracy 0.6128\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.525074 valid_accuracy 0.6428\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 0.590006 valid_accuracy 0.6282\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 0.587527 valid_accuracy 0.6272\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 0.561298 valid_accuracy 0.6314\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 0.532858 valid_accuracy 0.632\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.570719 valid_accuracy 0.6248\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 0.571967 valid_accuracy 0.635\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 0.519366 valid_accuracy 0.6342\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 0.504583 valid_accuracy 0.6406\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.486665 valid_accuracy 0.634\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.574598 valid_accuracy 0.6272\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.577468 valid_accuracy 0.636\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 0.536778 valid_accuracy 0.6286\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 0.54118 valid_accuracy 0.6302\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.53534 valid_accuracy 0.6286\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.544595 valid_accuracy 0.6378\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 0.52002 valid_accuracy 0.6444\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 0.546159 valid_accuracy 0.6266\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 0.526918 valid_accuracy 0.6374\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.482752 valid_accuracy 0.6404\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.51324 valid_accuracy 0.6422\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.514218 valid_accuracy 0.6462\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 0.52284 valid_accuracy 0.6274\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 0.56211 valid_accuracy 0.6248\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.478648 valid_accuracy 0.6414\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.496079 valid_accuracy 0.644\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 0.534042 valid_accuracy 0.6374\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 0.495387 valid_accuracy 0.635\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 0.57163 valid_accuracy 0.6178\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.469487 valid_accuracy 0.647\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.474701 valid_accuracy 0.6482\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 0.550609 valid_accuracy 0.6342\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.515582 valid_accuracy 0.6258\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 0.529058 valid_accuracy 0.631\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.490766 valid_accuracy 0.6344\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.487732 valid_accuracy 0.6466\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 0.534117 valid_accuracy 0.6354\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.510291 valid_accuracy 0.6258\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 0.49696 valid_accuracy 0.6402\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.456943 valid_accuracy 0.6442\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.519211 valid_accuracy 0.6316\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.552908 valid_accuracy 0.6328\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 0.510655 valid_accuracy 0.6264\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 0.498037 valid_accuracy 0.6424\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.437415 valid_accuracy 0.6456\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.50048 valid_accuracy 0.6334\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.575574 valid_accuracy 0.629\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.496934 valid_accuracy 0.63\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 0.501932 valid_accuracy 0.6396\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.432634 valid_accuracy 0.648\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.449913 valid_accuracy 0.6466\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.509113 valid_accuracy 0.6392\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.462163 valid_accuracy 0.6386\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 0.48545 valid_accuracy 0.6364\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.416311 valid_accuracy 0.6548\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.438204 valid_accuracy 0.6482\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.483244 valid_accuracy 0.641\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.477349 valid_accuracy 0.6294\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 0.477632 valid_accuracy 0.6328\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.402753 valid_accuracy 0.6522\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.432751 valid_accuracy 0.6524\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.515431 valid_accuracy 0.6322\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.51083 valid_accuracy 0.6152\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 0.457002 valid_accuracy 0.641\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.412302 valid_accuracy 0.6476\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.441924 valid_accuracy 0.6498\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.56859 valid_accuracy 0.6138\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.518258 valid_accuracy 0.6184\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 0.479481 valid_accuracy 0.6426\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.415007 valid_accuracy 0.6484\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.426845 valid_accuracy 0.6494\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.545095 valid_accuracy 0.6196\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.502679 valid_accuracy 0.6212\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 0.4845 valid_accuracy 0.6366\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.400511 valid_accuracy 0.6522\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.413961 valid_accuracy 0.6532\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.511474 valid_accuracy 0.6266\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.486944 valid_accuracy 0.6236\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 0.486674 valid_accuracy 0.6348\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.395711 valid_accuracy 0.6494\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.423583 valid_accuracy 0.649\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 0.474119 valid_accuracy 0.636\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 0.441086 valid_accuracy 0.633\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 0.467708 valid_accuracy 0.6376\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 0.387134 valid_accuracy 0.6512\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.423068 valid_accuracy 0.6468\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 0.479673 valid_accuracy 0.6314\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 0.428434 valid_accuracy 0.6362\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 0.453026 valid_accuracy 0.6372\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 0.392755 valid_accuracy 0.6432\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.422553 valid_accuracy 0.6464\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 0.491597 valid_accuracy 0.6232\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 0.418264 valid_accuracy 0.639\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 0.44016 valid_accuracy 0.6384\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 0.415377 valid_accuracy 0.6358\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.426679 valid_accuracy 0.6466\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 0.518902 valid_accuracy 0.616\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 0.446178 valid_accuracy 0.6296\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 0.429804 valid_accuracy 0.6426\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 0.481985 valid_accuracy 0.6128\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.463011 valid_accuracy 0.6378\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 0.494451 valid_accuracy 0.6232\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 0.51056 valid_accuracy 0.6214\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 0.49102 valid_accuracy 0.6288\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 0.461942 valid_accuracy 0.6322\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.502031 valid_accuracy 0.6268\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 0.496904 valid_accuracy 0.628\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 0.467708 valid_accuracy 0.6368\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 0.473773 valid_accuracy 0.6362\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss: 0.473672 valid_accuracy 0.6204\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.503464 valid_accuracy 0.6272\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 0.494015 valid_accuracy 0.6252\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 0.418501 valid_accuracy 0.6464\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 0.456615 valid_accuracy 0.6354\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 0.398756 valid_accuracy 0.6408\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.475281 valid_accuracy 0.6306\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 0.466489 valid_accuracy 0.6298\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 0.408023 valid_accuracy 0.6442\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 0.437332 valid_accuracy 0.6382\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 0.397708 valid_accuracy 0.6418\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.461954 valid_accuracy 0.6316\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 0.444244 valid_accuracy 0.6352\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 0.39273 valid_accuracy 0.6456\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 0.423891 valid_accuracy 0.636\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 0.392654 valid_accuracy 0.6448\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.457856 valid_accuracy 0.6318\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 0.444999 valid_accuracy 0.6324\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 0.385189 valid_accuracy 0.6454\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 0.420942 valid_accuracy 0.6366\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 0.400033 valid_accuracy 0.6422\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.437342 valid_accuracy 0.6318\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 0.439799 valid_accuracy 0.6296\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 0.37584 valid_accuracy 0.6466\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 0.429029 valid_accuracy 0.6344\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 0.416475 valid_accuracy 0.6418\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.415104 valid_accuracy 0.6372\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 0.428948 valid_accuracy 0.6286\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 0.388891 valid_accuracy 0.6446\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 0.449343 valid_accuracy 0.626\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 0.395482 valid_accuracy 0.6398\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.399172 valid_accuracy 0.6424\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 0.465185 valid_accuracy 0.6184\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 0.423464 valid_accuracy 0.6362\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 0.438531 valid_accuracy 0.6298\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 0.413369 valid_accuracy 0.636\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.426633 valid_accuracy 0.6378\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 0.473615 valid_accuracy 0.6212\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 0.391924 valid_accuracy 0.6404\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 0.431628 valid_accuracy 0.6266\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 0.371901 valid_accuracy 0.6388\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.457574 valid_accuracy 0.6342\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 0.478101 valid_accuracy 0.6196\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 0.361505 valid_accuracy 0.6404\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 0.364501 valid_accuracy 0.6488\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 0.406094 valid_accuracy 0.6292\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.427728 valid_accuracy 0.6408\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 0.457376 valid_accuracy 0.6234\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 0.367482 valid_accuracy 0.6418\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 0.361747 valid_accuracy 0.6468\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 0.386454 valid_accuracy 0.6282\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.394254 valid_accuracy 0.6502\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 0.43796 valid_accuracy 0.6246\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 0.379678 valid_accuracy 0.6348\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 0.352443 valid_accuracy 0.6492\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 0.370306 valid_accuracy 0.6308\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.395367 valid_accuracy 0.649\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 0.419675 valid_accuracy 0.6286\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 0.380814 valid_accuracy 0.6322\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 0.352029 valid_accuracy 0.6492\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 0.361998 valid_accuracy 0.6336\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.398033 valid_accuracy 0.6462\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 0.408291 valid_accuracy 0.628\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 0.372456 valid_accuracy 0.6296\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 0.347759 valid_accuracy 0.65\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 0.355743 valid_accuracy 0.6352\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.398599 valid_accuracy 0.6428\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 0.405931 valid_accuracy 0.6264\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 0.361582 valid_accuracy 0.6304\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 0.33847 valid_accuracy 0.648\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 0.34602 valid_accuracy 0.6348\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6394172430038452\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSKTFsYiIiIhIpMWxiIiIiEj0/wF+kOI5B2HU6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14dbf20128>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
