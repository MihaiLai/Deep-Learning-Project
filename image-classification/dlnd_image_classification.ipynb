{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:56, 3.01MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 10:\n",
      "Image - Min Value: 24 Max Value: 130\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 4 Name: deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njQl6AGjs/wCMj7S6AwR1rgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15627109b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 10\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    a = 0\n",
    "    b = 1\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (x - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "    one_hot_x = lb.transform(x)\n",
    "    #print (x)\n",
    "    #print (one_hot_x)\n",
    "    return one_hot_x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    #print (image_shape)\n",
    "    return tf.placeholder(tf.float32,shape =[None,*image_shape],name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    #print (n_classes)\n",
    "    return tf.placeholder(tf.float32,shape =[None,10],name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "   \n",
    "    return tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 32, 32, 5), dtype=float32) 10 (2, 2) (4, 4) (2, 2) (2, 2)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    #print(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    filter_size_height = conv_ksize[0]\n",
    "    filter_size_width = conv_ksize[1]\n",
    "    x_tensor_depth = int(x_tensor.get_shape()[3])\n",
    "    #print(type(x_tensor_depth))\n",
    "    #print (filter_size_height,filter_size_width,x_tensor_depth)\n",
    "    weight = tf.Variable(tf.truncated_normal([filter_size_height, filter_size_width, x_tensor_depth, conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, [1,conv_strides[0],conv_strides[1],1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,[1,pool_ksize[0],pool_ksize[1],1],[1,pool_strides[0],pool_strides[1],1],padding='SAME')\n",
    "    print (x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # Implement Function\n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs, activation=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 32, 32, 3), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 8, 8, 64), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 2, 2, 64), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"Placeholder:0\", shape=(?, 32, 32, 3), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"MaxPool_3:0\", shape=(?, 8, 8, 64), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Tensor(\"MaxPool_4:0\", shape=(?, 2, 2, 64), dtype=float32) 64 (5, 5) (2, 2) (3, 3) (2, 2)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (5, 5)\n",
    "    conv_strides = (2,2)\n",
    "    pool_ksize = (3, 3)\n",
    "    pool_strides = (2,2)\n",
    "    n_class = 10\n",
    "    \n",
    "    conv_layer = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    #  Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat_x = flatten(conv_layer)\n",
    "    \n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    full_layer = fully_conn(flat_x,32)\n",
    "    full_layer = fully_conn(full_layer,n_class)\n",
    "    #full_layer = fully_conn(full_layer,n_class)\n",
    "    \n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    drop_layer = tf.layers.dropout(full_layer,rate = keep_prob)\n",
    "    out = output(drop_layer, n_class)\n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    train_feed_dict = {\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability     \n",
    "    }\n",
    "    session.run(optimizer, train_feed_dict)\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "    valid_accuracy = session.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob:1.})\n",
    "    print ('loss:',loss,'valid_accuracy',valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.2812 valid_accuracy 0.1372\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.23577 valid_accuracy 0.1764\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.20132 valid_accuracy 0.1848\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.1594 valid_accuracy 0.214\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.10487 valid_accuracy 0.2358\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 2.05085 valid_accuracy 0.2404\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.98948 valid_accuracy 0.27\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.91858 valid_accuracy 0.2914\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.84836 valid_accuracy 0.3176\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.77946 valid_accuracy 0.351\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.71511 valid_accuracy 0.3722\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.658 valid_accuracy 0.3912\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.60247 valid_accuracy 0.4008\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.55548 valid_accuracy 0.4134\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.5005 valid_accuracy 0.4232\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.45616 valid_accuracy 0.436\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.43764 valid_accuracy 0.436\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.38336 valid_accuracy 0.4514\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.3639 valid_accuracy 0.4536\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.34768 valid_accuracy 0.457\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.31003 valid_accuracy 0.4674\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.25573 valid_accuracy 0.475\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.25165 valid_accuracy 0.472\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.19938 valid_accuracy 0.4898\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 1.22235 valid_accuracy 0.4752\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.14696 valid_accuracy 0.4996\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.15392 valid_accuracy 0.4902\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.12371 valid_accuracy 0.4958\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 1.14136 valid_accuracy 0.4894\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 1.11648 valid_accuracy 0.4984\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 1.07411 valid_accuracy 0.5104\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 1.06081 valid_accuracy 0.5084\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 1.069 valid_accuracy 0.5064\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 1.04392 valid_accuracy 0.518\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 1.00921 valid_accuracy 0.5102\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 1.09136 valid_accuracy 0.4886\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.990743 valid_accuracy 0.512\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.964252 valid_accuracy 0.524\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.965006 valid_accuracy 0.5186\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.955655 valid_accuracy 0.5194\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.930532 valid_accuracy 0.5204\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.909313 valid_accuracy 0.5244\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.896439 valid_accuracy 0.5224\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.879171 valid_accuracy 0.5212\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.863499 valid_accuracy 0.5236\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.867676 valid_accuracy 0.5166\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.899139 valid_accuracy 0.5124\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.854584 valid_accuracy 0.5154\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.853495 valid_accuracy 0.5172\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.83714 valid_accuracy 0.5252\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.791821 valid_accuracy 0.5272\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.860834 valid_accuracy 0.5032\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.796641 valid_accuracy 0.5136\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.77428 valid_accuracy 0.531\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.745886 valid_accuracy 0.5242\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.771044 valid_accuracy 0.5174\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.730462 valid_accuracy 0.5258\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.704708 valid_accuracy 0.5348\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.696742 valid_accuracy 0.5318\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.686567 valid_accuracy 0.5294\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.675746 valid_accuracy 0.5294\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.671243 valid_accuracy 0.5282\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.674962 valid_accuracy 0.5208\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.726325 valid_accuracy 0.5112\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.854484 valid_accuracy 0.4826\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.709667 valid_accuracy 0.5162\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.636329 valid_accuracy 0.5312\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.63687 valid_accuracy 0.529\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.627214 valid_accuracy 0.5296\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.62619 valid_accuracy 0.5268\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.631041 valid_accuracy 0.5196\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.610919 valid_accuracy 0.521\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.606681 valid_accuracy 0.524\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.573648 valid_accuracy 0.5304\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.575075 valid_accuracy 0.5188\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.553482 valid_accuracy 0.5278\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.568366 valid_accuracy 0.52\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.562278 valid_accuracy 0.5212\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.554206 valid_accuracy 0.5198\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.512109 valid_accuracy 0.5264\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.520451 valid_accuracy 0.5254\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.546786 valid_accuracy 0.5152\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.495418 valid_accuracy 0.5266\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.517565 valid_accuracy 0.5232\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.564846 valid_accuracy 0.5106\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.472161 valid_accuracy 0.5276\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.513958 valid_accuracy 0.5248\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.497346 valid_accuracy 0.5238\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.449501 valid_accuracy 0.531\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.464161 valid_accuracy 0.5248\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.441731 valid_accuracy 0.5266\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.416503 valid_accuracy 0.53\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.40604 valid_accuracy 0.5288\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.401161 valid_accuracy 0.5304\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.403064 valid_accuracy 0.5334\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.415217 valid_accuracy 0.5318\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.437143 valid_accuracy 0.5218\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.432331 valid_accuracy 0.5196\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.37336 valid_accuracy 0.5262\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.384925 valid_accuracy 0.5232\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.29453 valid_accuracy 0.1052\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 2.28128 valid_accuracy 0.1004\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 2.2771 valid_accuracy 0.1318\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 2.26385 valid_accuracy 0.1196\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 2.27141 valid_accuracy 0.1324\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.26824 valid_accuracy 0.138\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 2.24655 valid_accuracy 0.1312\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 2.23897 valid_accuracy 0.1234\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 2.24094 valid_accuracy 0.1368\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 2.24433 valid_accuracy 0.1384\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.22294 valid_accuracy 0.1364\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 2.20135 valid_accuracy 0.1282\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 2.17982 valid_accuracy 0.1312\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 2.17534 valid_accuracy 0.1202\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 2.19886 valid_accuracy 0.1988\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.15221 valid_accuracy 0.2032\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 2.1368 valid_accuracy 0.1716\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 2.10457 valid_accuracy 0.189\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 2.11202 valid_accuracy 0.1966\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 2.11277 valid_accuracy 0.203\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.06233 valid_accuracy 0.2286\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 2.02263 valid_accuracy 0.2284\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 1.98997 valid_accuracy 0.2282\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 1.94218 valid_accuracy 0.267\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 1.93145 valid_accuracy 0.2702\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 1.89185 valid_accuracy 0.2804\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 1.86528 valid_accuracy 0.2886\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 1.85886 valid_accuracy 0.28\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 1.79866 valid_accuracy 0.3286\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 1.77257 valid_accuracy 0.3198\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.73209 valid_accuracy 0.353\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 1.65618 valid_accuracy 0.3786\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 1.6106 valid_accuracy 0.3918\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 1.60239 valid_accuracy 0.3862\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 1.59407 valid_accuracy 0.3898\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.55514 valid_accuracy 0.3974\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 1.4853 valid_accuracy 0.4186\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 1.45989 valid_accuracy 0.43\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 1.42344 valid_accuracy 0.448\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 1.46267 valid_accuracy 0.4466\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.41635 valid_accuracy 0.4538\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 1.37608 valid_accuracy 0.4802\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 1.34341 valid_accuracy 0.4932\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 1.3088 valid_accuracy 0.4902\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 1.33498 valid_accuracy 0.5076\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.30385 valid_accuracy 0.5178\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 1.30335 valid_accuracy 0.5126\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 1.26647 valid_accuracy 0.514\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 1.23103 valid_accuracy 0.5222\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 1.25036 valid_accuracy 0.5272\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.23788 valid_accuracy 0.5324\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 1.29353 valid_accuracy 0.5132\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 1.20451 valid_accuracy 0.5258\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 1.17862 valid_accuracy 0.5384\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 1.19819 valid_accuracy 0.5394\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.20027 valid_accuracy 0.5378\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 1.2797 valid_accuracy 0.5162\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 1.17059 valid_accuracy 0.5278\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 1.15796 valid_accuracy 0.55\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 1.15973 valid_accuracy 0.549\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.14853 valid_accuracy 0.5554\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 1.21768 valid_accuracy 0.5256\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 1.10962 valid_accuracy 0.5542\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 1.1273 valid_accuracy 0.5614\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 1.11439 valid_accuracy 0.556\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.12427 valid_accuracy 0.563\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 1.17412 valid_accuracy 0.5356\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 1.07994 valid_accuracy 0.5642\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 1.10957 valid_accuracy 0.5674\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 1.07544 valid_accuracy 0.5684\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.10741 valid_accuracy 0.5622\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 1.1783 valid_accuracy 0.53\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 1.08883 valid_accuracy 0.5654\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 1.10433 valid_accuracy 0.5668\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 1.04994 valid_accuracy 0.5724\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.0669 valid_accuracy 0.5796\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 1.11852 valid_accuracy 0.5544\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 1.10536 valid_accuracy 0.5612\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 1.06973 valid_accuracy 0.5762\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 1.04655 valid_accuracy 0.572\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.04215 valid_accuracy 0.5866\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 1.0965 valid_accuracy 0.5838\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 1.04905 valid_accuracy 0.5746\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 1.02405 valid_accuracy 0.593\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 0.985441 valid_accuracy 0.5854\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.04255 valid_accuracy 0.5824\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 1.07953 valid_accuracy 0.58\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 1.01028 valid_accuracy 0.5884\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 1.00963 valid_accuracy 0.5932\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 0.96501 valid_accuracy 0.5892\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.02875 valid_accuracy 0.584\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 1.0627 valid_accuracy 0.5766\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 0.981341 valid_accuracy 0.5954\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 0.99678 valid_accuracy 0.594\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 0.954426 valid_accuracy 0.5898\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.03704 valid_accuracy 0.58\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 1.02056 valid_accuracy 0.584\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 0.975301 valid_accuracy 0.597\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 0.999735 valid_accuracy 0.5864\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 0.963249 valid_accuracy 0.5888\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 0.96702 valid_accuracy 0.6016\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 0.982492 valid_accuracy 0.6054\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 0.960261 valid_accuracy 0.5924\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 0.987604 valid_accuracy 0.5922\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 0.927227 valid_accuracy 0.5986\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 0.982512 valid_accuracy 0.597\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 0.973684 valid_accuracy 0.6054\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 0.931155 valid_accuracy 0.5962\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 0.95266 valid_accuracy 0.603\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 0.889706 valid_accuracy 0.604\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 0.954247 valid_accuracy 0.6058\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 0.956336 valid_accuracy 0.6114\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 0.911973 valid_accuracy 0.6046\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 0.928443 valid_accuracy 0.6066\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 0.869421 valid_accuracy 0.6082\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.931942 valid_accuracy 0.6076\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 0.938696 valid_accuracy 0.6162\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 0.89529 valid_accuracy 0.6056\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 0.905974 valid_accuracy 0.6112\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 0.846035 valid_accuracy 0.6166\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.914438 valid_accuracy 0.612\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 0.923544 valid_accuracy 0.6144\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 0.875857 valid_accuracy 0.6122\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 0.883428 valid_accuracy 0.618\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 0.825859 valid_accuracy 0.6238\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.897419 valid_accuracy 0.6178\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 0.907536 valid_accuracy 0.6186\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 0.861343 valid_accuracy 0.6162\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 0.860832 valid_accuracy 0.6222\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 0.805628 valid_accuracy 0.6222\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.882389 valid_accuracy 0.6192\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 0.891269 valid_accuracy 0.6252\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 0.850042 valid_accuracy 0.6196\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 0.843991 valid_accuracy 0.6222\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 0.789095 valid_accuracy 0.6258\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.869869 valid_accuracy 0.6204\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 0.876345 valid_accuracy 0.626\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 0.829911 valid_accuracy 0.6242\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 0.828802 valid_accuracy 0.6236\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 0.772561 valid_accuracy 0.6288\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.852565 valid_accuracy 0.6238\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 0.864022 valid_accuracy 0.6264\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 0.815055 valid_accuracy 0.629\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 0.811854 valid_accuracy 0.6264\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 0.757872 valid_accuracy 0.6278\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.834407 valid_accuracy 0.624\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 0.850931 valid_accuracy 0.6278\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 0.798315 valid_accuracy 0.6302\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 0.79802 valid_accuracy 0.6278\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 0.740098 valid_accuracy 0.632\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.818209 valid_accuracy 0.6268\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 0.838277 valid_accuracy 0.6292\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 0.786092 valid_accuracy 0.6304\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 0.786312 valid_accuracy 0.6286\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 0.722899 valid_accuracy 0.6352\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.798693 valid_accuracy 0.6314\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 0.831129 valid_accuracy 0.6256\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 0.775304 valid_accuracy 0.6328\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 0.770047 valid_accuracy 0.631\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 0.711454 valid_accuracy 0.6366\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.780217 valid_accuracy 0.6316\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 0.817758 valid_accuracy 0.6296\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 0.760226 valid_accuracy 0.6352\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 0.752071 valid_accuracy 0.6338\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 0.697646 valid_accuracy 0.6388\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.764636 valid_accuracy 0.634\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 0.809971 valid_accuracy 0.6304\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 0.749918 valid_accuracy 0.6354\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 0.74202 valid_accuracy 0.6378\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 0.685613 valid_accuracy 0.643\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.749014 valid_accuracy 0.6356\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 0.798788 valid_accuracy 0.632\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 0.744719 valid_accuracy 0.635\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 0.731685 valid_accuracy 0.6382\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 0.672524 valid_accuracy 0.6442\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.735032 valid_accuracy 0.6394\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 0.789207 valid_accuracy 0.631\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 0.735766 valid_accuracy 0.6338\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 0.723439 valid_accuracy 0.6356\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 0.665066 valid_accuracy 0.6424\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.726435 valid_accuracy 0.6352\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 0.797933 valid_accuracy 0.6234\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 0.737339 valid_accuracy 0.6292\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 0.718949 valid_accuracy 0.6372\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 0.676836 valid_accuracy 0.6362\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.721215 valid_accuracy 0.6374\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 0.770624 valid_accuracy 0.636\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 0.725751 valid_accuracy 0.6342\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 0.730274 valid_accuracy 0.6284\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 0.68449 valid_accuracy 0.6262\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.748481 valid_accuracy 0.6322\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 0.778909 valid_accuracy 0.6336\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 0.684963 valid_accuracy 0.6466\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 0.694549 valid_accuracy 0.6376\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 0.635681 valid_accuracy 0.645\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.724927 valid_accuracy 0.6322\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 0.779349 valid_accuracy 0.628\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 0.676133 valid_accuracy 0.6494\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 0.694669 valid_accuracy 0.6344\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 0.6236 valid_accuracy 0.6442\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.715983 valid_accuracy 0.6374\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 0.758848 valid_accuracy 0.634\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 0.674502 valid_accuracy 0.6412\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 0.695694 valid_accuracy 0.6318\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 0.63709 valid_accuracy 0.639\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.688516 valid_accuracy 0.6434\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 0.741526 valid_accuracy 0.6328\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 0.650892 valid_accuracy 0.6448\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 0.688881 valid_accuracy 0.6322\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 0.655754 valid_accuracy 0.6284\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.697355 valid_accuracy 0.6362\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 0.7209 valid_accuracy 0.6408\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 0.639272 valid_accuracy 0.6484\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 0.704797 valid_accuracy 0.6256\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 0.632212 valid_accuracy 0.6358\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.679471 valid_accuracy 0.6424\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 0.705232 valid_accuracy 0.6418\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 0.64232 valid_accuracy 0.6462\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 0.679359 valid_accuracy 0.632\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 0.615119 valid_accuracy 0.638\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.665004 valid_accuracy 0.6476\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 0.696661 valid_accuracy 0.643\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 0.635157 valid_accuracy 0.643\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 0.660006 valid_accuracy 0.6368\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 0.609961 valid_accuracy 0.639\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.64499 valid_accuracy 0.6474\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 0.687503 valid_accuracy 0.6414\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 0.633952 valid_accuracy 0.6366\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 0.653346 valid_accuracy 0.639\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 0.594193 valid_accuracy 0.642\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.640174 valid_accuracy 0.645\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 0.671163 valid_accuracy 0.6444\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 0.634708 valid_accuracy 0.6324\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 0.675002 valid_accuracy 0.6282\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 0.577225 valid_accuracy 0.6464\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.636203 valid_accuracy 0.6468\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 0.665019 valid_accuracy 0.6492\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 0.616981 valid_accuracy 0.6404\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 0.668315 valid_accuracy 0.6302\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 0.568141 valid_accuracy 0.6434\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.62638 valid_accuracy 0.649\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 0.658437 valid_accuracy 0.648\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 0.596161 valid_accuracy 0.6426\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 0.647223 valid_accuracy 0.6382\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 0.555839 valid_accuracy 0.6478\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.622079 valid_accuracy 0.6532\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 0.662878 valid_accuracy 0.6438\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 0.610921 valid_accuracy 0.6438\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 0.627873 valid_accuracy 0.6366\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 0.563818 valid_accuracy 0.6464\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.632035 valid_accuracy 0.6514\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 0.653288 valid_accuracy 0.6496\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 0.63394 valid_accuracy 0.6358\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 0.651349 valid_accuracy 0.6262\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 0.587705 valid_accuracy 0.6376\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.637131 valid_accuracy 0.6466\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 0.664999 valid_accuracy 0.642\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 0.60705 valid_accuracy 0.6366\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 0.709541 valid_accuracy 0.6084\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 0.624605 valid_accuracy 0.6282\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.659687 valid_accuracy 0.6422\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 0.691925 valid_accuracy 0.6356\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 0.598608 valid_accuracy 0.636\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 0.682223 valid_accuracy 0.6164\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 0.644509 valid_accuracy 0.6142\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.650079 valid_accuracy 0.6402\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 0.649987 valid_accuracy 0.6496\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 0.594372 valid_accuracy 0.6376\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 0.671119 valid_accuracy 0.6178\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 0.60492 valid_accuracy 0.62\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.628253 valid_accuracy 0.6544\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 0.672474 valid_accuracy 0.6404\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 0.557795 valid_accuracy 0.6548\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 0.583346 valid_accuracy 0.6482\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 0.573034 valid_accuracy 0.6378\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.61698 valid_accuracy 0.6518\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 0.659273 valid_accuracy 0.6432\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 0.548169 valid_accuracy 0.6526\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 0.56956 valid_accuracy 0.649\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 0.555204 valid_accuracy 0.64\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.616555 valid_accuracy 0.6508\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 0.639669 valid_accuracy 0.6522\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 0.523221 valid_accuracy 0.6548\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 0.571326 valid_accuracy 0.6444\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 0.529631 valid_accuracy 0.6438\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.623366 valid_accuracy 0.6438\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 0.621892 valid_accuracy 0.6556\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 0.51445 valid_accuracy 0.6564\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 0.566999 valid_accuracy 0.6422\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 0.509743 valid_accuracy 0.6424\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.611925 valid_accuracy 0.6426\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 0.610653 valid_accuracy 0.6552\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 0.506634 valid_accuracy 0.653\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 0.569527 valid_accuracy 0.6384\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 0.495589 valid_accuracy 0.6436\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.606983 valid_accuracy 0.6416\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 0.604818 valid_accuracy 0.6526\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 0.505653 valid_accuracy 0.6502\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 0.571804 valid_accuracy 0.6352\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 0.486968 valid_accuracy 0.6466\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.590666 valid_accuracy 0.6444\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 0.62172 valid_accuracy 0.6426\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 0.539905 valid_accuracy 0.64\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 0.605135 valid_accuracy 0.6174\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 0.517474 valid_accuracy 0.6432\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.552163 valid_accuracy 0.6632\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 0.588923 valid_accuracy 0.6524\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 0.509968 valid_accuracy 0.6448\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 0.554686 valid_accuracy 0.6382\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 0.505382 valid_accuracy 0.6452\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.544697 valid_accuracy 0.658\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 0.581307 valid_accuracy 0.6542\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 0.490927 valid_accuracy 0.6534\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 0.537113 valid_accuracy 0.6416\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 0.508774 valid_accuracy 0.6384\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.534737 valid_accuracy 0.6582\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 0.574922 valid_accuracy 0.6558\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 0.48053 valid_accuracy 0.6542\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 0.522335 valid_accuracy 0.646\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.511201 valid_accuracy 0.6352\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.526646 valid_accuracy 0.6582\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 0.569043 valid_accuracy 0.6552\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 0.471928 valid_accuracy 0.6572\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 0.504869 valid_accuracy 0.6492\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 0.51063 valid_accuracy 0.6342\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.520768 valid_accuracy 0.6538\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 0.568401 valid_accuracy 0.6546\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 0.472646 valid_accuracy 0.6522\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 0.495864 valid_accuracy 0.6492\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.503774 valid_accuracy 0.6366\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.521685 valid_accuracy 0.6476\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.567637 valid_accuracy 0.6532\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 0.468728 valid_accuracy 0.6514\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 0.490813 valid_accuracy 0.651\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.495923 valid_accuracy 0.6334\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.525393 valid_accuracy 0.6436\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 0.583369 valid_accuracy 0.6476\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 0.459459 valid_accuracy 0.6538\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 0.483091 valid_accuracy 0.6544\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.493742 valid_accuracy 0.632\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.515052 valid_accuracy 0.6468\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.602205 valid_accuracy 0.6372\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 0.460911 valid_accuracy 0.6502\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 0.476224 valid_accuracy 0.6492\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.510119 valid_accuracy 0.6258\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.523401 valid_accuracy 0.6398\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 0.60146 valid_accuracy 0.6412\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 0.506123 valid_accuracy 0.6358\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 0.493303 valid_accuracy 0.641\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.492808 valid_accuracy 0.6268\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.520712 valid_accuracy 0.6404\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 0.570021 valid_accuracy 0.6458\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.528818 valid_accuracy 0.6266\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 0.502592 valid_accuracy 0.6442\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.497074 valid_accuracy 0.6266\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.543333 valid_accuracy 0.6292\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 0.574691 valid_accuracy 0.6384\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.546292 valid_accuracy 0.616\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 0.550066 valid_accuracy 0.6314\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.482119 valid_accuracy 0.6248\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.497622 valid_accuracy 0.6452\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.567105 valid_accuracy 0.6426\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 0.467709 valid_accuracy 0.6452\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 0.508227 valid_accuracy 0.6404\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.467024 valid_accuracy 0.6456\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.546166 valid_accuracy 0.6404\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.591939 valid_accuracy 0.6332\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.442767 valid_accuracy 0.6534\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 0.491819 valid_accuracy 0.6404\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.446752 valid_accuracy 0.6462\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.509418 valid_accuracy 0.6446\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.550374 valid_accuracy 0.6422\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.429751 valid_accuracy 0.6576\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 0.475902 valid_accuracy 0.6446\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.430855 valid_accuracy 0.6472\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.490299 valid_accuracy 0.6456\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.521463 valid_accuracy 0.6448\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.426187 valid_accuracy 0.6584\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 0.467227 valid_accuracy 0.644\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.418874 valid_accuracy 0.6454\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.477741 valid_accuracy 0.647\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.50434 valid_accuracy 0.648\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.425387 valid_accuracy 0.6564\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 0.460807 valid_accuracy 0.6428\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.414003 valid_accuracy 0.6446\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.473713 valid_accuracy 0.6464\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.492373 valid_accuracy 0.651\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.420523 valid_accuracy 0.6604\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 0.448964 valid_accuracy 0.6446\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.415353 valid_accuracy 0.6446\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.469519 valid_accuracy 0.6454\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.481848 valid_accuracy 0.6504\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.418188 valid_accuracy 0.6564\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 0.448943 valid_accuracy 0.6426\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.416481 valid_accuracy 0.6424\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.468701 valid_accuracy 0.6446\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.481687 valid_accuracy 0.6494\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.40951 valid_accuracy 0.6542\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 0.452207 valid_accuracy 0.643\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.421383 valid_accuracy 0.6394\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.456044 valid_accuracy 0.6478\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 0.490782 valid_accuracy 0.6474\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 0.398676 valid_accuracy 0.6584\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 0.467121 valid_accuracy 0.6384\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 0.429771 valid_accuracy 0.6366\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.44338 valid_accuracy 0.65\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 0.489995 valid_accuracy 0.6488\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 0.395348 valid_accuracy 0.654\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 0.447725 valid_accuracy 0.641\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 0.426311 valid_accuracy 0.6414\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.449896 valid_accuracy 0.6482\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 0.483585 valid_accuracy 0.651\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 0.427184 valid_accuracy 0.6394\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 0.41304 valid_accuracy 0.6492\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 0.385523 valid_accuracy 0.6426\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.451509 valid_accuracy 0.646\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 0.548621 valid_accuracy 0.639\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 0.49638 valid_accuracy 0.6204\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 0.510734 valid_accuracy 0.626\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 0.402589 valid_accuracy 0.6312\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.517292 valid_accuracy 0.6448\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 0.518958 valid_accuracy 0.6446\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 0.412097 valid_accuracy 0.6458\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 0.443183 valid_accuracy 0.6424\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 0.487619 valid_accuracy 0.6186\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.453577 valid_accuracy 0.6524\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 0.489933 valid_accuracy 0.6432\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 0.417672 valid_accuracy 0.6462\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 0.42042 valid_accuracy 0.6446\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss: 0.468181 valid_accuracy 0.616\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.450134 valid_accuracy 0.648\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 0.479277 valid_accuracy 0.6438\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 0.39703 valid_accuracy 0.6464\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 0.400669 valid_accuracy 0.6526\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 0.461842 valid_accuracy 0.6136\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.453588 valid_accuracy 0.6464\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 0.486678 valid_accuracy 0.6428\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 0.393199 valid_accuracy 0.6448\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 0.391255 valid_accuracy 0.6504\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 0.471036 valid_accuracy 0.6064\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.438931 valid_accuracy 0.6508\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 0.485235 valid_accuracy 0.643\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 0.415049 valid_accuracy 0.6338\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 0.389264 valid_accuracy 0.6436\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 0.460291 valid_accuracy 0.6108\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.498059 valid_accuracy 0.6388\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 0.504201 valid_accuracy 0.638\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 0.436359 valid_accuracy 0.629\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 0.456498 valid_accuracy 0.6256\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 0.425061 valid_accuracy 0.6274\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.556872 valid_accuracy 0.6216\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 0.493359 valid_accuracy 0.6438\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 0.434638 valid_accuracy 0.6232\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 0.485119 valid_accuracy 0.618\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 0.445615 valid_accuracy 0.6248\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.561353 valid_accuracy 0.6092\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 0.589198 valid_accuracy 0.6276\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 0.475399 valid_accuracy 0.614\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 0.421071 valid_accuracy 0.6404\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 0.452042 valid_accuracy 0.6254\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.507468 valid_accuracy 0.64\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 0.527578 valid_accuracy 0.6344\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 0.42006 valid_accuracy 0.6364\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 0.436168 valid_accuracy 0.6358\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 0.431654 valid_accuracy 0.6302\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.464556 valid_accuracy 0.6444\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 0.486895 valid_accuracy 0.6416\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 0.408853 valid_accuracy 0.637\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 0.424609 valid_accuracy 0.6376\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 0.387492 valid_accuracy 0.641\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.421301 valid_accuracy 0.6478\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 0.499284 valid_accuracy 0.6372\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 0.415587 valid_accuracy 0.6372\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 0.429251 valid_accuracy 0.6374\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 0.413559 valid_accuracy 0.636\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.455477 valid_accuracy 0.6326\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 0.54679 valid_accuracy 0.6296\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 0.419216 valid_accuracy 0.6332\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 0.413351 valid_accuracy 0.637\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 0.425606 valid_accuracy 0.6276\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.444273 valid_accuracy 0.6334\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 0.543948 valid_accuracy 0.6274\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 0.402663 valid_accuracy 0.643\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 0.397547 valid_accuracy 0.6384\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 0.39582 valid_accuracy 0.6294\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.41563 valid_accuracy 0.636\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 0.544296 valid_accuracy 0.6286\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 0.422324 valid_accuracy 0.635\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 0.391248 valid_accuracy 0.6396\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 0.402468 valid_accuracy 0.6276\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.448666 valid_accuracy 0.632\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 0.44429 valid_accuracy 0.6466\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 0.430757 valid_accuracy 0.6322\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 0.49856 valid_accuracy 0.625\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 0.414925 valid_accuracy 0.6334\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.408409 valid_accuracy 0.639\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 0.486295 valid_accuracy 0.6446\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 0.382535 valid_accuracy 0.6386\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 0.396566 valid_accuracy 0.6422\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 0.372207 valid_accuracy 0.6326\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.633390462398529\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc178186be0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
